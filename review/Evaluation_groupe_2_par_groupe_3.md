---
title: Grille d’évaluation croisée
---

### Grille d’évaluation croisée

::: info
À remplir par chaque groupe pour chaque autre groupe, sur une échelle de 1 à 5, avec 5 = excellent). A remplir pour le **17 novembre** impérativement.
:::

==Groupe évaluateur== : groupe 3 (Sixtine, Emilie, Erwan) ==Groupe évalué== : groupe 2 (Riwal, Anna et Melina)

| **Critère** | **Sous-critères** | **Note (1-5)** | **Commentaires** |
|--------------|-----------------------------|--------------|--------------|
| **1. Pertinence du thème** | Le thème choisi est-il original et complémentaire aux enseignements existants ? | 4 | Le thème choisi est intéressant mais n'apporte pas de nouvelles approches. Cela dit la comparaison étiat pertinente. |
|  | L’ancrage en sciences du vivant est-il justifié et bien illustré ? | 4 | L'exemple choisi permet de comprendre la différence entre les environnements. |
| **2. Structure et clarté** | La présentation suit-elle une logique pédagogique (méthodologie → technique → application) ? | 5 | La présentation est bien structurée, approche très pédagogique, on comprend facilement |
|  | Le niveau de détail permet-il une bonne compréhension du sujet ? | 5 | Le sujet est clair, on comprend bien quels ont été les points de comparaison |
| **3. Qualité méthodologique** | Les concepts clés sont-ils expliqués de manière accessible mais rigoureuse ? | 5 | La définition d'une API est très clair, le reste est accessible |
|  | Les choix méthodologiques sont-ils argumentés ? | 4 | Le choix d'une comparaison plutôt que l'exploration d'une nouvelle méthode est justifié par le choix du sujet. |
| **4. Rigueur technique** | Le code (GitHub) est-il fonctionnel et commenté | 5 | Le code fonctionne bien et est bien commenté |
|  | Les résultats sont-ils reproductibles (par ex. y a t-il des graines pour générer les échantillons ?) | 5 | Présence d'une seed au début du code, les résultats sont reproductibles |
|  | Les données sont-elles bien documentées et adaptées à l’application ? | 5 | Oui, adaptées à la classification |
|  | Y a-t-il des erreurs techniques ou des approximations ? | 4 | Nous n'avons trouvé d'approximation. |
| **5. Ancrage applicatif** | L’application en sciences du vivant est-elle pertinente et bien illustrée ? | 2 | Le choix du sujet n'est pas tourné vers un exemple, l'exemple choisi est pris pour rentrer dans la catégorie sciences du vivant. La présentation est plus axée sur la comparaison. |
|  | Les limites de la méthode/thème sont-elles mentionnées ?? | 5 | Les limites de chaque méthodes sont mentionnées en comparaison aux autres. |
| **6. Qualité des supports** | Les slides sont-ils synthétiques et d'une forme adaptée ? | 4 | Les slides sont claires et structurées ce qui permet une bonne compréhension de l'information. En revanche il y a beaucoup de texte (lignes de code notamment). |
|  | Le dépôt GitHub est-il organisé (README, structure claire, données accessibles) ? | 5 | Le dépôt est clair. |
|  | Les documents partagés en amont (code, données, slides) sont-ils utilisables et complets pour une réutilisation ? | 4 | Les données pour le test ont été envoyées. |
|  | Les sources sont-elles citées et fiables ? | 5 | Ce sont les dites officiels. |
