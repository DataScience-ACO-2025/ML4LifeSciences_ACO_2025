{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ea5ac3",
   "metadata": {},
   "source": [
    "# \"Apport d'un modèle Bayésien lors de l'utilisation de réseaux de neuronnes pour la classification d'images\"\n",
    "\n",
    "Comparaison de 2 versions du modèle AlexNet pour la classification d'images, exemple sur un jeu d'images d'insectes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95b870",
   "metadata": {},
   "source": [
    "## Démarche et difficultés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d445f2a",
   "metadata": {},
   "source": [
    "### Rappel de la démarche projet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e55be",
   "metadata": {},
   "source": [
    "Notre projet porte sur des problématiques d'apprentissage profond avec des réseaux de neuronnes pour de la reconnaissance d'images. Il a pour but d'évaluer l'apport d'un modèle comportant des composantes bayésiennes par rapport à un modèle purement fréquentiste. En comparant les 2 modèles du point de vue de l'acuracy en fonction des ressources demandées (nombre d'image dans le jeu de donnée, temps d'entrainement du modèle, ressource GPU, ...) nous pourrons alors déterminer dans quels cas il est préférable d'utiliser l'une des 2 méthodes.\n",
    "\n",
    "Concrêtement, il s'agira de prendre le modèle AlexNet déjà pré-entrainé (en conservant les poids optenu à la suite de son entrainement sur ImageNet) et de le fine-tuner pour nos données. Il en sera fait de même pour le modèle bayésien pour permettre une comparaison ayant le plus de sens possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45636ebf",
   "metadata": {},
   "source": [
    "### Problèmes encontrés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebfc47",
   "metadata": {},
   "source": [
    "Pour ce projet, nous sommes passé par 3 étapes clés : \n",
    "- Mise en route : recherches documentaires/bibliographiques + établissement de la marche à suivre\n",
    "- CNN : compréhension, mise en place et résultats\n",
    "- BNN : compréhension, mise en place et résultats\n",
    "\n",
    "Lors de nos travaux, nous nous somme heurté à 2 difficultés majeures :   \n",
    "- Une difficulté de compréhension tout d'abord : Il nous fallait réellement et concrêtement comprendre les tenant et aboutissant d'un réseau de neurones pour pouvoir l'appliquer à notre cas.\n",
    "- Une difficulté technique par la suite : Une fois le fonctionnement du modèle définit, il nous fallait le faire fonctionner. La compréhension des objets manipulés et des espaces de départs et d'arrvée nous ont tout particulièrement occupé.\n",
    "\n",
    "Ces difficultés étaient attendu, et ont été dépassées pour l'étape CNN après un travail commun. Nous comptions ainsi faire de même pour la dernière étape : le BNN. Cependant, malgré les bases de la méthode solidement encrées et acquises via le travail sur le CNN, l'adaptation au cas du BNN ne s'est pas faites aussi facilement, ce qui était également attendu. Seulement pas à cette échelle. \n",
    "\n",
    "Opérationnelement, nous bloquons à obtenir un modèle avec une fonction de perte cohérente à un modèle entrainé et fine-tuné, et des performances dépassant le hazard. Cela traduit certainement un problèmes dans le calcul et la transmission des distributions de poids pour chaque couches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5c7b2",
   "metadata": {},
   "source": [
    "### Objectifs de ce document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4094fda0",
   "metadata": {},
   "source": [
    "Nos efforts ont certes drastiquement amélioré le modèle, mais ses performances restent insuffisants. Ainsi, les résultats restent très insatisfaisant et ne nous laissent pas l'opportunité de comparer pleinenemtn avec le modèle CNN malgré nos efforts. Nous avons donc pris la décision de compiler l'ensemble de notre travail à la fois de réflexion et de modification techniques pour tenter d'aboutir à un modèle utilisable, et ce pour mieux vous faire comprendre l'établissement de modèles d'apprentissages profonds en bayésiens, type BNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176be6b7",
   "metadata": {},
   "source": [
    "## Mise en oeuvre et changements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d09557",
   "metadata": {},
   "source": [
    "### Premier pas : package Pytorch bayesianCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1166159",
   "metadata": {},
   "source": [
    "### Modifications :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca8c7",
   "metadata": {},
   "source": [
    "Modification 1 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dd4a5",
   "metadata": {},
   "source": [
    "La première difficulté que nous avons rencontré c'est qu'en bayésien les poids sont des distributions et non pas des nombres.\n",
    "Donc les poids pré-entrainés de AlexNet sur ImageNet ne sont pas adapter.\n",
    "Pour cela on a fait la fonction transfer_weights_to_bayesian.\n",
    "Avec cette fonction, les poids sont choisis pour être des gaussienne centrées sur les poids d'AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transférer les poids vers les moyennes (mu) des distributions bayésiennes\n",
    "def transfer_weights_to_bayesian(bayesian_model, pretrained_model):\n",
    "    \"\"\"\n",
    "    Transfère les poids d'un modèle classique vers les moyennes (mu) \n",
    "    des distributions bayésiennes\n",
    "    \"\"\"\n",
    "    # Mapping des couches conv\n",
    "    conv_mapping = {\n",
    "        'conv1': 0,  # pretrained.features[0] -> bayesian.conv1\n",
    "        'conv2': 3,  # pretrained.features[3] -> bayesian.conv2\n",
    "        'conv3': 6,  # pretrained.features[6] -> bayesian.conv3\n",
    "        'conv4': 8,  # pretrained.features[8] -> bayesian.conv4\n",
    "        'conv5': 10, # pretrained.features[10] -> bayesian.conv5\n",
    "    }\n",
    "    \n",
    "    # Transférer les couches de convolution\n",
    "    for bay_name, pre_idx in conv_mapping.items():\n",
    "        bay_layer = getattr(bayesian_model, bay_name)\n",
    "        pre_layer = pretrained_model.features[pre_idx]\n",
    "        \n",
    "        # Copier weight vers weight_mu\n",
    "        bay_layer.W_mu.data.copy_(pre_layer.weight.data)\n",
    "        if pre_layer.bias is not None:\n",
    "            bay_layer.bias_mu.data.copy_(pre_layer.bias.data)\n",
    "        \n",
    "        print(f\"✅ Transféré {bay_name} depuis features[{pre_idx}]\")\n",
    "    \n",
    "    print(\"✅ Transfert terminé ! Les moyennes bayésiennes sont initialisées avec AlexNet pré-entraîné\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc0e98",
   "metadata": {},
   "source": [
    "Modification 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e77d92",
   "metadata": {},
   "source": [
    "Le problème avec cette solution, c'est qu'on avait une loss très importante par rapport à ce qu'on avait avec le CNN.\n",
    "La loss était de l'ordre du milion et augmentait au cours des époches.\n",
    "Pour corriger cela nous avons tester des priors différents. Nous avons retenu des petites valeurs de sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = {\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.01),  # plus petit écart pour mu\n",
    "    'posterior_rho_initial': (-4, 0.01),  # plus petit écart pour rho → sigma initial plus faible\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b95ee7",
   "metadata": {},
   "source": [
    "Modification 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bdd16",
   "metadata": {},
   "source": [
    "Nous avons mis un très petit learning rate car sinon la loss augmentait trop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37403d",
   "metadata": {},
   "source": [
    "Modification 4 : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67a895",
   "metadata": {},
   "source": [
    "Nous avons fait un travail de normalisation en centrant, réduisant puis en appliquant une fonction arcsin à nos données pour réduire les valeurs extrêmes en ajoutant une borne à -3 et 3.\n",
    "Sans ça, la loss était tellement grande qu'elle n'étant pas mesurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Transformation commune ===\n",
    "mean = [0.0, 0.0, 0.0]\n",
    "std = [1.0, 1.0, 1.0]\n",
    "\n",
    "class ArcSinhTransform:\n",
    "    def __call__(self, x):\n",
    "        return torch.asinh(x)\n",
    "\n",
    "class ClipTransform:\n",
    "    def __init__(self, min_val=-3.0, max_val=3.0):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    def __call__(self, x):\n",
    "        return torch.clamp(x, self.min_val, self.max_val)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((189, 189)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    ArcSinhTransform(),\n",
    "    ClipTransform(-3.0, 3.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae547584",
   "metadata": {},
   "source": [
    "Modification 5 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd22e9",
   "metadata": {},
   "source": [
    "On a beaucoup diminuer le poid du kl et on l'a rendu adaptatif. Si on ne faisait pas cela, la divergence de kullback-Liebler était trop importante et l'algorithme voulais juste minimiser ce critère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6934ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_weight = min(0.0005, (epoch + 1) * 0.0005 / 5)  # KL annealing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
