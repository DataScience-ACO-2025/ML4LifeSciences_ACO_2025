[
  {
    "objectID": "programme.html",
    "href": "programme.html",
    "title": "Programme",
    "section": "",
    "text": "Order By\n       Default\n         \n          Description\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nApport d‚Äôun mod√®le Bay√©sien lors de l‚Äôutilisation de r√©seaux de neuronnes pour la classification d‚Äôimages\n\n\nheure - passage\n\n\nComparaison de 2 ‚Äòversions‚Äô du mod√®le AlexNet pour la classification d‚Äôimages, exemple sur un jeu d‚Äôimages d‚Äôinsectes\n\n\n\n\n\n\nNov 21, 2025\n\n\nRoattino Thibault, Belle Louis, Rosa Mat√©o\n\n\n\n\n\n\n  \n\n\n\n\nClassification d‚Äôimages - une comparaison entre R et Python\n\n\nheure - passage\n\n\nComparaison m√©thologiques de l‚Äôutilisation de Keras sur R et Python et Pytorch sur Python dans le cadre d‚Äôune classification d‚Äôimages √† 2 classes\n\n\n\n\n\n\nNov 21, 2025\n\n\nCl√©ment Melina, Le Moan Delalande Riwal, Mathieu Anna\n\n\n\n\n\n\n  \n\n\n\n\nLe Machine Learning dans l‚Äôagriculture\n\n\nheure - passage\n\n\n\n\n\n\n\n\n\nNov 21, 2025\n\n\nLef√®vre Vadim, Naux Emilie, Julliard Valentine\n\n\n\n\n\n\n  \n\n\n\n\nLien vers le code github\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPr√©dire la structure secondaire des prot√©ines : du signal biologique aux approches d‚Äôapprentissage avanc√©es\n\n\nheure - passage\n\n\nPr√©diction de la structure secondaire √† partir de la s√©quence, en √©valuant l‚Äôimpact de diff√©rents descripteurs biologiques et des m√©thodes Random Forest, CNN et mod√®les de NLP sp√©cialis√©s du type BERT.\n\n\n\n\n\n\nNov 21, 2025\n\n\nBOULET Faustine, BEAUFILS Constance, PLACIER Mo√Øse\n\n\n\n\n\n\n  \n\n\n\n\nTitre du projet\n\n\nheure - passage\n\n\npetite description\n\n\n\n\n\n\nNov 21, 2025\n\n\nNom Prenom, Nom Prenom, Nom Prenom\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projets/Projet_exemple.html",
    "href": "projets/Projet_exemple.html",
    "title": "Crop Type Classification",
    "section": "",
    "text": "This project focuses on mapping crop types at the pixel level using multispectral satellite image time series. By leveraging both temporal and spatial information through a hybrid CNN architecture, we aim to accurately classify agricultural parcels across diverse French landscapes. The approach combines 1D convolutions for temporal encoding and a simplified U-Net for spatial segmentation.\n\n\n\n\n\n\nFor ground truth values, we utilize the 2023 Graphical Parcel Register (RPG) as the reference source. This dataset, provided as a shapefile, consists of polygons that precisely delineate agricultural parcels. Each polygon corresponds to an individual field and contains essential information: a unique identifier, its surface area measured in hectares (HA), and the identifier of the predominant crop grown on it during the 2023 cultivation season. The area covered is the entire contry with a total number of 9 797 405 parcels.\n\nonline visualisation\ndownloading link\n\n\n\n\nWe needed to clean the predominant crop identifier from the Registre Parcellaire Graphique (RPG) data. While the RPG encoding is useful for the Common Agricultural Policy (PAC), it isn‚Äôt ideal for our classification task.\nFor instance, the RPG distinguishes between ‚Äúsweet maize‚Äù and ‚Äúmaize‚Äù with separate crop IDs. However, these two share a nearly identical spectral signature, making their separation irrelevant for our classification purposes. This initial RPG classification also results in an excessively large number of classes.\nBy using the classes defined by Turkoglu et al., we leverage an expert-crafted set of more meaningful and spectrally distinct crop categories. This approach effectively addresses the over-granularity of the RPG, merging spectrally similar crops (like sweet maize and maize) into single, relevant categories, and significantly reducing the total number of classes to a practical size for our classification.\nWe also reprojected the CRS of parcel geometries (Lambert 93 : EPSG:2154) to match Sentinel-2‚Äôs projection system (WGS84: reproject each tile to its native UTM zone ), guaranting accurate overlay.\n\n\n\n\nWe rely on Sentinel-2 Level-2A (Surface Reflectance)\nWe leverages 4 spectral bands from Sentinel-2 satellites to perform crop detection. These bands allow us to extract detailed information about vegetation based on light reflectance at specific wavelengths.\n\n\n\n\n\n\n\n\n\n\n\n\nBand\nName\nResolution\nWavelength (S2A / S2B)\nDescription\n\n\n\n\nB2\nBlue\n10 m\n496.6 / 492.1 nm\nChlorophyll detection, cloud cover analysis\n\n\nB3\nGreen\n10 m\n560 / 559 nm\nVegetation contrast, plant health analysis\n\n\nB4\nRed\n10 m\n664.5 / 665 nm\nNDVI calculation, vegetation growth tracking\n\n\nB8\nNIR\n10 m\n835.1 / 833 nm\nBiomass detection, distinguishes soil vs vegetation\n\n\n\n\n\n\nTo effectively manage clouds in our satellite imagery, we leverage the Scene Classification Layer (SCL) provided by Sentinel-2. The SCL is a band within the Sentinel-2 Level-2A product that classifies each pixel based on its content (e.g., cloud, shadow, vegetation, water, snow). We use this layer to mask out all pixels not classified as vegetation, bare soil, or water (SCL classes 4 to 7), ensuring that clouds, cloud shadows, and other atmospheric artifacts are excluded from our analysis.\nFollowing the SCL masking, we compute a pixel-wise median for each month. Since each month provides between 6 and 15 images per band for a given area, the median composite is highly robust. We choose the median over the mean because it is significantly less impacted by outliers that might persist even after SCL masking (e.g., residual cloud edges or noise). This process also reduces the probability of having ‚Äúabsent pixels‚Äù due to clouds in our monthly composites.\nWhile median compositing greatly minimizes cloud-induced gaps, some might still occur. To address these, we implemented a pixel-wise temporal interpolation strategy:\n\nFor a missing pixel value in a given month: The new value is calculated as the mean of the corresponding pixel‚Äôs value from the previous month and the next month.\nFor missing values in the first month of the time series: We use the next available monthly value for that pixel.\nFor missing values in the last month of the time series: We use the last available previous monthly value for that pixel.\n\nAlthough this interpolation involves a nasty function with many for loops, it proves to be quite powerful in generating a complete and continuous time series of satellite imagery.\n\n\n\n\nDue to the extensive size of the covered area in France, we had to reduce the dataset used for modeling. To achieve this, we selected specific zones representing distinct agricultural landscapes, characterized by predominant crops and farming practices influenced by varying pedoclimatic conditions.\nThe following table lists these key agricultural zones across France, alongside nearby towns for geographic reference:\n\n\n\n\n\n\n\n\n\nZone\nRegion\nNearby Town (Map Reference)\nNotes\n\n\n\n\nNord-Picardie\nHauts-de-France\nSaint-Quentin (Aisne)\nSurrounded by large-scale crops (wheat, sugar beet, potatoes)\n\n\nParis Basin\n√éle-de-France / Centre\nChartres (Eure-et-Loir)\nHeart of the Beauce, vast cereal plains\n\n\nBrittany / Pays de la Loire\nBrittany / Vend√©e\nVitr√© (Ille-et-Vilaine)\nMixed zone: livestock, silage maize, hedgerows\n\n\nSouthwest\nNouvelle-Aquitaine\nAuch (Gers)\nCereal polyculture, maize, sunflower\n\n\nSoutheast\nProvence, Rh√¥ne-Alpes\nCarpentras (Vaucluse)\nVineyards, orchards, greenhouse vegetable farming\n\n\nMassif Central\nAuvergne\nRiom (Puy-de-D√¥me)\nLimagne plain: polyculture on volcanic plains\n\n\nAlsace / Lorraine\nGrand Est\nColmar (Haut-Rhin)\nHillside vineyards + lowland crop farming\n\n\nMediterranean\nOccitanie, PACA\nB√©ziers (H√©rault)\nVineyards, olive trees, vegetable crops, dry climate\n\n\n\nWe used ESA WorldCover (10 m resolution, global) to identify highly cultivated areas for sampling. From these representative landscapes, we selected five 10¬†km√ó10¬†km zones around each. This resulted in a total dataset covering 40 zones (8 regions √ó 5 zones each), each spanning 100¬†km^2.\nWe then extracted all parcels from the 2023 RPG that intersect with these zones, totaling 130,000 parcels with an average size of 1.5 HA. And retrieve computed median satellites images of teh studied zones for each month via the google earth engine API.\nDespite this effort to ensure geographical diversity, we still face a highly imbalanced class distribution across crop types ‚Äî some crops are vastly overrepresented while others have very few examples.\nThe figure below illustrates this imbalance (note the logarithmic scale on the y-axis).\nThis skew can significantly affect evaluation metrics: in particular, accuracy may be misleading, as the model can achieve high accuracy by favoring dominant classes. Therefore, we also report macro-averaged metrics (precision, recall, F1-score) that treat all classes equally, regardless of frequency.\n\n\n\nFor each of the eight agricultural regions described above, we selected one 10√ó10 km zone out of the five available to serve as the test dataset. So 20% of the dataset is held out for testing, and that each region is represented in the test set.\nThis allows us to evaluate the model‚Äôs ability to generalize to completely unseen geographic areas across diverse agro-climatic contexts.\nThe remaining 32 zones were used for training and validation. To optimize model performance while managing computational cost, we adopted a 3-fold cross-validation (CV) approach on the training set. A higher number of folds (e.g., K &gt; 3) was avoided due to the long training times associated with deep learning models on large spatial-temporal datasets.\nThis spatially-aware split avoide any data leakage between training and test areas."
  },
  {
    "objectID": "projets/Projet_exemple.html#dataset-description",
    "href": "projets/Projet_exemple.html#dataset-description",
    "title": "Crop Type Classification",
    "section": "",
    "text": "For ground truth values, we utilize the 2023 Graphical Parcel Register (RPG) as the reference source. This dataset, provided as a shapefile, consists of polygons that precisely delineate agricultural parcels. Each polygon corresponds to an individual field and contains essential information: a unique identifier, its surface area measured in hectares (HA), and the identifier of the predominant crop grown on it during the 2023 cultivation season. The area covered is the entire contry with a total number of 9 797 405 parcels.\n\nonline visualisation\ndownloading link\n\n\n\n\nWe needed to clean the predominant crop identifier from the Registre Parcellaire Graphique (RPG) data. While the RPG encoding is useful for the Common Agricultural Policy (PAC), it isn‚Äôt ideal for our classification task.\nFor instance, the RPG distinguishes between ‚Äúsweet maize‚Äù and ‚Äúmaize‚Äù with separate crop IDs. However, these two share a nearly identical spectral signature, making their separation irrelevant for our classification purposes. This initial RPG classification also results in an excessively large number of classes.\nBy using the classes defined by Turkoglu et al., we leverage an expert-crafted set of more meaningful and spectrally distinct crop categories. This approach effectively addresses the over-granularity of the RPG, merging spectrally similar crops (like sweet maize and maize) into single, relevant categories, and significantly reducing the total number of classes to a practical size for our classification.\nWe also reprojected the CRS of parcel geometries (Lambert 93 : EPSG:2154) to match Sentinel-2‚Äôs projection system (WGS84: reproject each tile to its native UTM zone ), guaranting accurate overlay.\n\n\n\n\nWe rely on Sentinel-2 Level-2A (Surface Reflectance)\nWe leverages 4 spectral bands from Sentinel-2 satellites to perform crop detection. These bands allow us to extract detailed information about vegetation based on light reflectance at specific wavelengths.\n\n\n\n\n\n\n\n\n\n\n\n\nBand\nName\nResolution\nWavelength (S2A / S2B)\nDescription\n\n\n\n\nB2\nBlue\n10 m\n496.6 / 492.1 nm\nChlorophyll detection, cloud cover analysis\n\n\nB3\nGreen\n10 m\n560 / 559 nm\nVegetation contrast, plant health analysis\n\n\nB4\nRed\n10 m\n664.5 / 665 nm\nNDVI calculation, vegetation growth tracking\n\n\nB8\nNIR\n10 m\n835.1 / 833 nm\nBiomass detection, distinguishes soil vs vegetation\n\n\n\n\n\n\nTo effectively manage clouds in our satellite imagery, we leverage the Scene Classification Layer (SCL) provided by Sentinel-2. The SCL is a band within the Sentinel-2 Level-2A product that classifies each pixel based on its content (e.g., cloud, shadow, vegetation, water, snow). We use this layer to mask out all pixels not classified as vegetation, bare soil, or water (SCL classes 4 to 7), ensuring that clouds, cloud shadows, and other atmospheric artifacts are excluded from our analysis.\nFollowing the SCL masking, we compute a pixel-wise median for each month. Since each month provides between 6 and 15 images per band for a given area, the median composite is highly robust. We choose the median over the mean because it is significantly less impacted by outliers that might persist even after SCL masking (e.g., residual cloud edges or noise). This process also reduces the probability of having ‚Äúabsent pixels‚Äù due to clouds in our monthly composites.\nWhile median compositing greatly minimizes cloud-induced gaps, some might still occur. To address these, we implemented a pixel-wise temporal interpolation strategy:\n\nFor a missing pixel value in a given month: The new value is calculated as the mean of the corresponding pixel‚Äôs value from the previous month and the next month.\nFor missing values in the first month of the time series: We use the next available monthly value for that pixel.\nFor missing values in the last month of the time series: We use the last available previous monthly value for that pixel.\n\nAlthough this interpolation involves a nasty function with many for loops, it proves to be quite powerful in generating a complete and continuous time series of satellite imagery.\n\n\n\n\nDue to the extensive size of the covered area in France, we had to reduce the dataset used for modeling. To achieve this, we selected specific zones representing distinct agricultural landscapes, characterized by predominant crops and farming practices influenced by varying pedoclimatic conditions.\nThe following table lists these key agricultural zones across France, alongside nearby towns for geographic reference:\n\n\n\n\n\n\n\n\n\nZone\nRegion\nNearby Town (Map Reference)\nNotes\n\n\n\n\nNord-Picardie\nHauts-de-France\nSaint-Quentin (Aisne)\nSurrounded by large-scale crops (wheat, sugar beet, potatoes)\n\n\nParis Basin\n√éle-de-France / Centre\nChartres (Eure-et-Loir)\nHeart of the Beauce, vast cereal plains\n\n\nBrittany / Pays de la Loire\nBrittany / Vend√©e\nVitr√© (Ille-et-Vilaine)\nMixed zone: livestock, silage maize, hedgerows\n\n\nSouthwest\nNouvelle-Aquitaine\nAuch (Gers)\nCereal polyculture, maize, sunflower\n\n\nSoutheast\nProvence, Rh√¥ne-Alpes\nCarpentras (Vaucluse)\nVineyards, orchards, greenhouse vegetable farming\n\n\nMassif Central\nAuvergne\nRiom (Puy-de-D√¥me)\nLimagne plain: polyculture on volcanic plains\n\n\nAlsace / Lorraine\nGrand Est\nColmar (Haut-Rhin)\nHillside vineyards + lowland crop farming\n\n\nMediterranean\nOccitanie, PACA\nB√©ziers (H√©rault)\nVineyards, olive trees, vegetable crops, dry climate\n\n\n\nWe used ESA WorldCover (10 m resolution, global) to identify highly cultivated areas for sampling. From these representative landscapes, we selected five 10¬†km√ó10¬†km zones around each. This resulted in a total dataset covering 40 zones (8 regions √ó 5 zones each), each spanning 100¬†km^2.\nWe then extracted all parcels from the 2023 RPG that intersect with these zones, totaling 130,000 parcels with an average size of 1.5 HA. And retrieve computed median satellites images of teh studied zones for each month via the google earth engine API.\nDespite this effort to ensure geographical diversity, we still face a highly imbalanced class distribution across crop types ‚Äî some crops are vastly overrepresented while others have very few examples.\nThe figure below illustrates this imbalance (note the logarithmic scale on the y-axis).\nThis skew can significantly affect evaluation metrics: in particular, accuracy may be misleading, as the model can achieve high accuracy by favoring dominant classes. Therefore, we also report macro-averaged metrics (precision, recall, F1-score) that treat all classes equally, regardless of frequency.\n\n\n\nFor each of the eight agricultural regions described above, we selected one 10√ó10 km zone out of the five available to serve as the test dataset. So 20% of the dataset is held out for testing, and that each region is represented in the test set.\nThis allows us to evaluate the model‚Äôs ability to generalize to completely unseen geographic areas across diverse agro-climatic contexts.\nThe remaining 32 zones were used for training and validation. To optimize model performance while managing computational cost, we adopted a 3-fold cross-validation (CV) approach on the training set. A higher number of folds (e.g., K &gt; 3) was avoided due to the long training times associated with deep learning models on large spatial-temporal datasets.\nThis spatially-aware split avoide any data leakage between training and test areas."
  },
  {
    "objectID": "projets/Projet_exemple.html#deep-learning-approach",
    "href": "projets/Projet_exemple.html#deep-learning-approach",
    "title": "Crop Type Classification",
    "section": "Deep Learning approach :",
    "text": "Deep Learning approach :\nOur model classifies each pixel of a multispectral time-series image into crop types by combining temporal and spatial feature extraction in a hybrid CNN architecture.\nInput: X ‚àà ‚Ñù1 ‚Äî a batch of multispectral sequences with 4 channels (e.g., Red, Green, Blue, NIR), T time steps, and spatial dimensions H√óW.\n\nStep 1: Temporal Encoding (Pixel-wise)\nReshape input to [B √ó H √ó W, C, T]. Apply a 1D CNN independently on each pixel‚Äôs temporal sequence. This captures temporal patterns per pixel across spectral bands and outputs D-dimensional embeddings.\n\n\nStep 2: Reshape to Spatial Grid\nReshape back to [B, D, H, W], forming a pseudo-image from the temporal embeddings.\n\n\nStep 3: Spatial Encoding (Image-wise)\nPass through a 2D CNN backbone (a simplified U-Net). This captures spatial context and relationships between neighboring pixels.\n\n\nStep 4: Classification\nThe final output is per-pixel class logits with shape [B, num_classes, H, W].\n\n\nResults :\nGlobal Accuracy = The ratio of correctly classified pixels over the total number of pixels\nPrecision, Recall, and F1-score (Macro) = averages across all classes (i.e., unweighted mean over classes):\n\n\n\nMetric\nScore\n\n\n\n\nAverage Loss\n0.5222\n\n\nGlobal Accuracy\n0.8721\n\n\nMacro Precision\n0.4099\n\n\nMacro Recall\n0.3544\n\n\nMacro F1-score\n0.3516"
  },
  {
    "objectID": "projets/Projet_exemple.html#xgboost-approach-pixel-wise-classification",
    "href": "projets/Projet_exemple.html#xgboost-approach-pixel-wise-classification",
    "title": "Crop Type Classification",
    "section": "XGBoost approach (Pixel-wise Classification)",
    "text": "XGBoost approach (Pixel-wise Classification)\nIn addition to the deep learning model, we implemented a classical machine learning pipeline using XGBoost to classify pixels individually based on their temporal and spectral profiles.\nWe use a flattened pixel-wise data. Each input vector represents the spectral evolution of a pixel over time.\n\nResults :\n\n\n\nMetric\nScore\n\n\n\n\nAverage Loss\n0.7358\n\n\nGlobal Accuracy\n0.8045\n\n\nMacro Precision\n0.3543\n\n\nMacro Recall\n0.2303\n\n\nMacro F1-score\n0.2524"
  },
  {
    "objectID": "projets/Projet_exemple.html#biblio",
    "href": "projets/Projet_exemple.html#biblio",
    "title": "Crop Type Classification",
    "section": "biblio",
    "text": "biblio\nMehmet Ozgur Turkoglu, Stefano D‚ÄôAronco, Gregor Perich, Frank Liebisch, Constantin Streit, Konrad Schindler, Jan Dirk Wegner, Crop mapping from image time series: Deep learning with multi-scale label hierarchies, Remote Sensing of Environment, Volume 264,2021,112603,ISSN 0034-4257, DOI GitHub"
  },
  {
    "objectID": "projets/Projet_exemple.html#authors",
    "href": "projets/Projet_exemple.html#authors",
    "title": "Crop Type Classification",
    "section": "Authors",
    "text": "Authors\nGiovanni Setaro - Github\nNo√© Coursimaux - Github\nMo√Øse Placier - Github"
  },
  {
    "objectID": "projets/Projet_exemple.html#footnotes",
    "href": "projets/Projet_exemple.html#footnotes",
    "title": "Crop Type Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nB, C=4, T, H, W‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A propos",
    "section": "",
    "text": "L‚ÄôInstitut Agro Rennes-Angers est un √©tablissement public d‚Äôenseignement sup√©rieur et de recherche en agronomie, environnement et alimentation. Il forme des ing√©nieurs et chercheurs capables de relever les d√©fis li√©s √† la durabilit√© des syst√®mes agricoles et alimentaires.\nLe Master Math√©matiques Appliqu√©es, Statistique est co-accr√©dit√©e par plusieurs √©tablissements d‚Äôenseignement sup√©rieur rennais, notament l‚ÄôUniversit√© Rennes 2 et l‚Äô√âcole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation (ENSAI).\nAu sein de l‚ÄôInstitut Agro Rennes-Angers, les √©tudiants suivent le parcours Data Science pour la biologie, qui vise √† d√©velopper des comp√©tences avanc√©es en analyse de donn√©es et en mod√©lisation statistique appliqu√©es aux domaines de l‚Äôagriculture, de l‚Äôagroalimentaire, des sciences de l‚Äôenvironnement et de la biologie."
  },
  {
    "objectID": "about.html#responsables-p√©agogiques",
    "href": "about.html#responsables-p√©agogiques",
    "title": "A propos",
    "section": "Responsables p√©agogiques :",
    "text": "Responsables p√©agogiques :\nResponsable de la sp√©cialisation : S√©bastien L√™ ‚Äî sebastien.le@institut-agro.fr\nResponsable du master : David Causeur ‚Äî david.causeur@institut-agro.fr"
  },
  {
    "objectID": "about.html#promo-2025",
    "href": "about.html#promo-2025",
    "title": "A propos",
    "section": "Promo 2025",
    "text": "Promo 2025\nLes √©tudiants de la sp√©cialisation Sciences des Donn√©es Biologiques pr√©sentent leurs travaux dans le cadre de la conf√©rence Machine Learning pour la Science du Vivant.\n\n\n Dupont Alice Etudiant ing√©nieur Agronome Institut Agro Rennes\nGitHub ‚Ä¢ LinkedIn\n\n\n\nMartin Hugo\nUniversit√© Rennes 2\nInstitut Agro Rennes\nGitHub ‚Ä¢ LinkedIn\n\n\n Placier Mo√Øse Etudiant ing√©nieur Agronome - ENSAT Institut Agro Rennes\nGitHub ‚Ä¢ LinkedIn\n\n\n Durand Emma Etudiant ing√©nieur Agroalimentaire Institut Agro Rennes GitHub ‚Ä¢ LinkedIn\n\n\n Placier Mo√Øse Etudiant ing√©nieur Agronome - ENSAT Institut Agro Rennes\nGitHub ‚Ä¢ LinkedIn\n\n\n Durand Emma Etudiant ing√©nieur Agroalimentaire Institut Agro Rennes GitHub ‚Ä¢ LinkedIn"
  },
  {
    "objectID": "projets/Projet_exemple_2.html",
    "href": "projets/Projet_exemple_2.html",
    "title": "cancer prediction",
    "section": "",
    "text": "This project focuses on mapping crop types at the pixel level using multispectral satellite image time series. By leveraging both temporal and spatial information through a hybrid CNN architecture, we aim to accurately classify agricultural parcels across diverse French landscapes. The approach combines 1D convolutions for temporal encoding and a simplified U-Net for spatial segmentation.\n\n\n\n\n\n\nFor ground truth values, we utilize the 2023 Graphical Parcel Register (RPG) as the reference source. This dataset, provided as a shapefile, consists of polygons that precisely delineate agricultural parcels. Each polygon corresponds to an individual field and contains essential information: a unique identifier, its surface area measured in hectares (HA), and the identifier of the predominant crop grown on it during the 2023 cultivation season. The area covered is the entire contry with a total number of 9 797 405 parcels.\n\nonline visualisation\ndownloading link\n\n\n\n\nWe needed to clean the predominant crop identifier from the Registre Parcellaire Graphique (RPG) data. While the RPG encoding is useful for the Common Agricultural Policy (PAC), it isn‚Äôt ideal for our classification task.\nFor instance, the RPG distinguishes between ‚Äúsweet maize‚Äù and ‚Äúmaize‚Äù with separate crop IDs. However, these two share a nearly identical spectral signature, making their separation irrelevant for our classification purposes. This initial RPG classification also results in an excessively large number of classes.\nBy using the classes defined by Turkoglu et al., we leverage an expert-crafted set of more meaningful and spectrally distinct crop categories. This approach effectively addresses the over-granularity of the RPG, merging spectrally similar crops (like sweet maize and maize) into single, relevant categories, and significantly reducing the total number of classes to a practical size for our classification.\nWe also reprojected the CRS of parcel geometries (Lambert 93 : EPSG:2154) to match Sentinel-2‚Äôs projection system (WGS84: reproject each tile to its native UTM zone ), guaranting accurate overlay.\n\n\n\n\nWe rely on Sentinel-2 Level-2A (Surface Reflectance)\nWe leverages 4 spectral bands from Sentinel-2 satellites to perform crop detection. These bands allow us to extract detailed information about vegetation based on light reflectance at specific wavelengths.\n\n\n\n\n\n\n\n\n\n\n\n\nBand\nName\nResolution\nWavelength (S2A / S2B)\nDescription\n\n\n\n\nB2\nBlue\n10 m\n496.6 / 492.1 nm\nChlorophyll detection, cloud cover analysis\n\n\nB3\nGreen\n10 m\n560 / 559 nm\nVegetation contrast, plant health analysis\n\n\nB4\nRed\n10 m\n664.5 / 665 nm\nNDVI calculation, vegetation growth tracking\n\n\nB8\nNIR\n10 m\n835.1 / 833 nm\nBiomass detection, distinguishes soil vs vegetation\n\n\n\n\n\n\nTo effectively manage clouds in our satellite imagery, we leverage the Scene Classification Layer (SCL) provided by Sentinel-2. The SCL is a band within the Sentinel-2 Level-2A product that classifies each pixel based on its content (e.g., cloud, shadow, vegetation, water, snow). We use this layer to mask out all pixels not classified as vegetation, bare soil, or water (SCL classes 4 to 7), ensuring that clouds, cloud shadows, and other atmospheric artifacts are excluded from our analysis.\nFollowing the SCL masking, we compute a pixel-wise median for each month. Since each month provides between 6 and 15 images per band for a given area, the median composite is highly robust. We choose the median over the mean because it is significantly less impacted by outliers that might persist even after SCL masking (e.g., residual cloud edges or noise). This process also reduces the probability of having ‚Äúabsent pixels‚Äù due to clouds in our monthly composites.\nWhile median compositing greatly minimizes cloud-induced gaps, some might still occur. To address these, we implemented a pixel-wise temporal interpolation strategy:\n\nFor a missing pixel value in a given month: The new value is calculated as the mean of the corresponding pixel‚Äôs value from the previous month and the next month.\nFor missing values in the first month of the time series: We use the next available monthly value for that pixel.\nFor missing values in the last month of the time series: We use the last available previous monthly value for that pixel.\n\nAlthough this interpolation involves a nasty function with many for loops, it proves to be quite powerful in generating a complete and continuous time series of satellite imagery.\n\n\n\n\nDue to the extensive size of the covered area in France, we had to reduce the dataset used for modeling. To achieve this, we selected specific zones representing distinct agricultural landscapes, characterized by predominant crops and farming practices influenced by varying pedoclimatic conditions.\nThe following table lists these key agricultural zones across France, alongside nearby towns for geographic reference:\n\n\n\n\n\n\n\n\n\nZone\nRegion\nNearby Town (Map Reference)\nNotes\n\n\n\n\nNord-Picardie\nHauts-de-France\nSaint-Quentin (Aisne)\nSurrounded by large-scale crops (wheat, sugar beet, potatoes)\n\n\nParis Basin\n√éle-de-France / Centre\nChartres (Eure-et-Loir)\nHeart of the Beauce, vast cereal plains\n\n\nBrittany / Pays de la Loire\nBrittany / Vend√©e\nVitr√© (Ille-et-Vilaine)\nMixed zone: livestock, silage maize, hedgerows\n\n\nSouthwest\nNouvelle-Aquitaine\nAuch (Gers)\nCereal polyculture, maize, sunflower\n\n\nSoutheast\nProvence, Rh√¥ne-Alpes\nCarpentras (Vaucluse)\nVineyards, orchards, greenhouse vegetable farming\n\n\nMassif Central\nAuvergne\nRiom (Puy-de-D√¥me)\nLimagne plain: polyculture on volcanic plains\n\n\nAlsace / Lorraine\nGrand Est\nColmar (Haut-Rhin)\nHillside vineyards + lowland crop farming\n\n\nMediterranean\nOccitanie, PACA\nB√©ziers (H√©rault)\nVineyards, olive trees, vegetable crops, dry climate\n\n\n\nWe used ESA WorldCover (10 m resolution, global) to identify highly cultivated areas for sampling. From these representative landscapes, we selected five 10¬†km√ó10¬†km zones around each. This resulted in a total dataset covering 40 zones (8 regions √ó 5 zones each), each spanning 100¬†km^2.\nWe then extracted all parcels from the 2023 RPG that intersect with these zones, totaling 130,000 parcels with an average size of 1.5 HA. And retrieve computed median satellites images of teh studied zones for each month via the google earth engine API.\nDespite this effort to ensure geographical diversity, we still face a highly imbalanced class distribution across crop types ‚Äî some crops are vastly overrepresented while others have very few examples.\nThe figure below illustrates this imbalance (note the logarithmic scale on the y-axis).\nThis skew can significantly affect evaluation metrics: in particular, accuracy may be misleading, as the model can achieve high accuracy by favoring dominant classes. Therefore, we also report macro-averaged metrics (precision, recall, F1-score) that treat all classes equally, regardless of frequency.\n\n\n\nFor each of the eight agricultural regions described above, we selected one 10√ó10 km zone out of the five available to serve as the test dataset. So 20% of the dataset is held out for testing, and that each region is represented in the test set.\nThis allows us to evaluate the model‚Äôs ability to generalize to completely unseen geographic areas across diverse agro-climatic contexts.\nThe remaining 32 zones were used for training and validation. To optimize model performance while managing computational cost, we adopted a 3-fold cross-validation (CV) approach on the training set. A higher number of folds (e.g., K &gt; 3) was avoided due to the long training times associated with deep learning models on large spatial-temporal datasets.\nThis spatially-aware split avoide any data leakage between training and test areas."
  },
  {
    "objectID": "projets/Projet_exemple_2.html#dataset-description",
    "href": "projets/Projet_exemple_2.html#dataset-description",
    "title": "cancer prediction",
    "section": "",
    "text": "For ground truth values, we utilize the 2023 Graphical Parcel Register (RPG) as the reference source. This dataset, provided as a shapefile, consists of polygons that precisely delineate agricultural parcels. Each polygon corresponds to an individual field and contains essential information: a unique identifier, its surface area measured in hectares (HA), and the identifier of the predominant crop grown on it during the 2023 cultivation season. The area covered is the entire contry with a total number of 9 797 405 parcels.\n\nonline visualisation\ndownloading link\n\n\n\n\nWe needed to clean the predominant crop identifier from the Registre Parcellaire Graphique (RPG) data. While the RPG encoding is useful for the Common Agricultural Policy (PAC), it isn‚Äôt ideal for our classification task.\nFor instance, the RPG distinguishes between ‚Äúsweet maize‚Äù and ‚Äúmaize‚Äù with separate crop IDs. However, these two share a nearly identical spectral signature, making their separation irrelevant for our classification purposes. This initial RPG classification also results in an excessively large number of classes.\nBy using the classes defined by Turkoglu et al., we leverage an expert-crafted set of more meaningful and spectrally distinct crop categories. This approach effectively addresses the over-granularity of the RPG, merging spectrally similar crops (like sweet maize and maize) into single, relevant categories, and significantly reducing the total number of classes to a practical size for our classification.\nWe also reprojected the CRS of parcel geometries (Lambert 93 : EPSG:2154) to match Sentinel-2‚Äôs projection system (WGS84: reproject each tile to its native UTM zone ), guaranting accurate overlay.\n\n\n\n\nWe rely on Sentinel-2 Level-2A (Surface Reflectance)\nWe leverages 4 spectral bands from Sentinel-2 satellites to perform crop detection. These bands allow us to extract detailed information about vegetation based on light reflectance at specific wavelengths.\n\n\n\n\n\n\n\n\n\n\n\n\nBand\nName\nResolution\nWavelength (S2A / S2B)\nDescription\n\n\n\n\nB2\nBlue\n10 m\n496.6 / 492.1 nm\nChlorophyll detection, cloud cover analysis\n\n\nB3\nGreen\n10 m\n560 / 559 nm\nVegetation contrast, plant health analysis\n\n\nB4\nRed\n10 m\n664.5 / 665 nm\nNDVI calculation, vegetation growth tracking\n\n\nB8\nNIR\n10 m\n835.1 / 833 nm\nBiomass detection, distinguishes soil vs vegetation\n\n\n\n\n\n\nTo effectively manage clouds in our satellite imagery, we leverage the Scene Classification Layer (SCL) provided by Sentinel-2. The SCL is a band within the Sentinel-2 Level-2A product that classifies each pixel based on its content (e.g., cloud, shadow, vegetation, water, snow). We use this layer to mask out all pixels not classified as vegetation, bare soil, or water (SCL classes 4 to 7), ensuring that clouds, cloud shadows, and other atmospheric artifacts are excluded from our analysis.\nFollowing the SCL masking, we compute a pixel-wise median for each month. Since each month provides between 6 and 15 images per band for a given area, the median composite is highly robust. We choose the median over the mean because it is significantly less impacted by outliers that might persist even after SCL masking (e.g., residual cloud edges or noise). This process also reduces the probability of having ‚Äúabsent pixels‚Äù due to clouds in our monthly composites.\nWhile median compositing greatly minimizes cloud-induced gaps, some might still occur. To address these, we implemented a pixel-wise temporal interpolation strategy:\n\nFor a missing pixel value in a given month: The new value is calculated as the mean of the corresponding pixel‚Äôs value from the previous month and the next month.\nFor missing values in the first month of the time series: We use the next available monthly value for that pixel.\nFor missing values in the last month of the time series: We use the last available previous monthly value for that pixel.\n\nAlthough this interpolation involves a nasty function with many for loops, it proves to be quite powerful in generating a complete and continuous time series of satellite imagery.\n\n\n\n\nDue to the extensive size of the covered area in France, we had to reduce the dataset used for modeling. To achieve this, we selected specific zones representing distinct agricultural landscapes, characterized by predominant crops and farming practices influenced by varying pedoclimatic conditions.\nThe following table lists these key agricultural zones across France, alongside nearby towns for geographic reference:\n\n\n\n\n\n\n\n\n\nZone\nRegion\nNearby Town (Map Reference)\nNotes\n\n\n\n\nNord-Picardie\nHauts-de-France\nSaint-Quentin (Aisne)\nSurrounded by large-scale crops (wheat, sugar beet, potatoes)\n\n\nParis Basin\n√éle-de-France / Centre\nChartres (Eure-et-Loir)\nHeart of the Beauce, vast cereal plains\n\n\nBrittany / Pays de la Loire\nBrittany / Vend√©e\nVitr√© (Ille-et-Vilaine)\nMixed zone: livestock, silage maize, hedgerows\n\n\nSouthwest\nNouvelle-Aquitaine\nAuch (Gers)\nCereal polyculture, maize, sunflower\n\n\nSoutheast\nProvence, Rh√¥ne-Alpes\nCarpentras (Vaucluse)\nVineyards, orchards, greenhouse vegetable farming\n\n\nMassif Central\nAuvergne\nRiom (Puy-de-D√¥me)\nLimagne plain: polyculture on volcanic plains\n\n\nAlsace / Lorraine\nGrand Est\nColmar (Haut-Rhin)\nHillside vineyards + lowland crop farming\n\n\nMediterranean\nOccitanie, PACA\nB√©ziers (H√©rault)\nVineyards, olive trees, vegetable crops, dry climate\n\n\n\nWe used ESA WorldCover (10 m resolution, global) to identify highly cultivated areas for sampling. From these representative landscapes, we selected five 10¬†km√ó10¬†km zones around each. This resulted in a total dataset covering 40 zones (8 regions √ó 5 zones each), each spanning 100¬†km^2.\nWe then extracted all parcels from the 2023 RPG that intersect with these zones, totaling 130,000 parcels with an average size of 1.5 HA. And retrieve computed median satellites images of teh studied zones for each month via the google earth engine API.\nDespite this effort to ensure geographical diversity, we still face a highly imbalanced class distribution across crop types ‚Äî some crops are vastly overrepresented while others have very few examples.\nThe figure below illustrates this imbalance (note the logarithmic scale on the y-axis).\nThis skew can significantly affect evaluation metrics: in particular, accuracy may be misleading, as the model can achieve high accuracy by favoring dominant classes. Therefore, we also report macro-averaged metrics (precision, recall, F1-score) that treat all classes equally, regardless of frequency.\n\n\n\nFor each of the eight agricultural regions described above, we selected one 10√ó10 km zone out of the five available to serve as the test dataset. So 20% of the dataset is held out for testing, and that each region is represented in the test set.\nThis allows us to evaluate the model‚Äôs ability to generalize to completely unseen geographic areas across diverse agro-climatic contexts.\nThe remaining 32 zones were used for training and validation. To optimize model performance while managing computational cost, we adopted a 3-fold cross-validation (CV) approach on the training set. A higher number of folds (e.g., K &gt; 3) was avoided due to the long training times associated with deep learning models on large spatial-temporal datasets.\nThis spatially-aware split avoide any data leakage between training and test areas."
  },
  {
    "objectID": "projets/Projet_exemple_2.html#deep-learning-approach",
    "href": "projets/Projet_exemple_2.html#deep-learning-approach",
    "title": "cancer prediction",
    "section": "Deep Learning approach :",
    "text": "Deep Learning approach :\nOur model classifies each pixel of a multispectral time-series image into crop types by combining temporal and spatial feature extraction in a hybrid CNN architecture.\nInput: X ‚àà ‚Ñù1 ‚Äî a batch of multispectral sequences with 4 channels (e.g., Red, Green, Blue, NIR), T time steps, and spatial dimensions H√óW.\n\nStep 1: Temporal Encoding (Pixel-wise)\nReshape input to [B √ó H √ó W, C, T]. Apply a 1D CNN independently on each pixel‚Äôs temporal sequence. This captures temporal patterns per pixel across spectral bands and outputs D-dimensional embeddings.\n\n\nStep 2: Reshape to Spatial Grid\nReshape back to [B, D, H, W], forming a pseudo-image from the temporal embeddings.\n\n\nStep 3: Spatial Encoding (Image-wise)\nPass through a 2D CNN backbone (a simplified U-Net). This captures spatial context and relationships between neighboring pixels.\n\n\nStep 4: Classification\nThe final output is per-pixel class logits with shape [B, num_classes, H, W].\n\n\nResults :\nGlobal Accuracy = The ratio of correctly classified pixels over the total number of pixels\nPrecision, Recall, and F1-score (Macro) = averages across all classes (i.e., unweighted mean over classes):\n\n\n\nMetric\nScore\n\n\n\n\nAverage Loss\n0.5222\n\n\nGlobal Accuracy\n0.8721\n\n\nMacro Precision\n0.4099\n\n\nMacro Recall\n0.3544\n\n\nMacro F1-score\n0.3516"
  },
  {
    "objectID": "projets/Projet_exemple_2.html#xgboost-approach-pixel-wise-classification",
    "href": "projets/Projet_exemple_2.html#xgboost-approach-pixel-wise-classification",
    "title": "cancer prediction",
    "section": "XGBoost approach (Pixel-wise Classification)",
    "text": "XGBoost approach (Pixel-wise Classification)\nIn addition to the deep learning model, we implemented a classical machine learning pipeline using XGBoost to classify pixels individually based on their temporal and spectral profiles.\nWe use a flattened pixel-wise data. Each input vector represents the spectral evolution of a pixel over time.\n\nResults :\n\n\n\nMetric\nScore\n\n\n\n\nAverage Loss\n0.7358\n\n\nGlobal Accuracy\n0.8045\n\n\nMacro Precision\n0.3543\n\n\nMacro Recall\n0.2303\n\n\nMacro F1-score\n0.2524"
  },
  {
    "objectID": "projets/Projet_exemple_2.html#biblio",
    "href": "projets/Projet_exemple_2.html#biblio",
    "title": "cancer prediction",
    "section": "biblio",
    "text": "biblio\nMehmet Ozgur Turkoglu, Stefano D‚ÄôAronco, Gregor Perich, Frank Liebisch, Constantin Streit, Konrad Schindler, Jan Dirk Wegner, Crop mapping from image time series: Deep learning with multi-scale label hierarchies, Remote Sensing of Environment, Volume 264,2021,112603,ISSN 0034-4257, DOI GitHub"
  },
  {
    "objectID": "projets/Projet_exemple_2.html#authors",
    "href": "projets/Projet_exemple_2.html#authors",
    "title": "cancer prediction",
    "section": "Authors",
    "text": "Authors\nGiovanni Setaro - Github\nNo√© Coursimaux - Github\nMo√Øse Placier - Github"
  },
  {
    "objectID": "projets/Projet_exemple_2.html#footnotes",
    "href": "projets/Projet_exemple_2.html#footnotes",
    "title": "cancer prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nB, C=4, T, H, W‚Ü©Ô∏é"
  },
  {
    "objectID": "projets/Template_presentation_projets.html",
    "href": "projets/Template_presentation_projets.html",
    "title": "Titre du projet",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/Template_presentation_projets.html#lien-vers-le-code-github",
    "href": "projets/Template_presentation_projets.html#lien-vers-le-code-github",
    "title": "Titre du projet",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/Template_presentation_projets.html#lien-vers-les-slides",
    "href": "projets/Template_presentation_projets.html#lien-vers-les-slides",
    "title": "Titre du projet",
    "section": "Lien vers les slides",
    "text": "Lien vers les slides\n\nSlides"
  },
  {
    "objectID": "projets/Template_presentation_projets.html#description-projet",
    "href": "projets/Template_presentation_projets.html#description-projet",
    "title": "Titre du projet",
    "section": "description Projet",
    "text": "description Projet\nblablabla"
  },
  {
    "objectID": "infos.html",
    "href": "infos.html",
    "title": "Infos pratiques",
    "section": "",
    "text": "Horaires :\nvendredi 21 novembre 2025 entre 13h30 et 16h30\n\n\nLieu\nSalle 5√®me ann√©e du B√¢timent 24\nInstitut Agro Rennes 65 Rue de Saint-Brieuc, 35042 Rennes.\n\n\nPlan d‚Äôacc√®s"
  },
  {
    "objectID": "index.html#pr√©sentation-de-l√©cole-dautomne",
    "href": "index.html#pr√©sentation-de-l√©cole-dautomne",
    "title": "3·µâ √©dition de la conf√©rence de Machine Learning en Sciences du Vivant",
    "section": "Pr√©sentation de l‚Äô√©cole d‚Äôautomne",
    "text": "Pr√©sentation de l‚Äô√©cole d‚Äôautomne\nOrganis√©e enti√®rement par les √©tudiants de Master 2 ‚Äì sp√©cialisation Sciences des Donn√©es, cette √©cole d‚Äôautomne annuelle est un rendez-vous incontournable !\nElle s‚Äôinscrit dans le cadre de l‚Äô√©valuation des modules Machine Learning et Computer Science for Big Data Approaches, et permet aux √©tudiants de restituer leurs projets sous forme de mini‚Äëcours. Chaque groupe pr√©sente ainsi un expos√© didactique, combinant th√©orie et pratique, pour partager ses m√©thodes et r√©sultats.\nC‚Äôest l‚Äôoccasion de mettre √† l‚Äôhonneur les travaux des √©tudiants et de d√©couvrir des approches modernes de Machine Learning et de Deep Learning appliqu√©es aux probl√©matiques des sciences du vivant.\n\nTh√®me 2025 ‚Äì Machine Learning pour la Science du Vivant\nPour cette 3·µâ √©dition,l‚Äô√©cole explore les apports du Machine Learning √† la compr√©hension, la mod√©lisation et la pr√©diction dans le champ du vivant : agriculture, environnement, √©cologie, sant√© ou biotechnologies.\nLes √©tudiants pr√©senteront des projets originaux illustrant la diversit√© des approches possibles et l‚Äôimpact concret de telles applications.\n\n\nLes acteurs de la conf√©rence\n\nEncadrants p√©dagogiques :\n\n\n\n Mathieu Emily\n\nProfesseur titulaire de statistique et responsable du d√©partement de recherche en statistique.\nLaboratoire : D√©partement statistique, IRMAR (UMR CNRS 6625)\nContact : mathieu.emily@agrocampus-ouest.fr\n\n\n Laetitia Chapel\n\nProfesseure titulaire en Computeur Science et chercheuse.\nLaboratoire : √âquipe OBELIX, IRISA (UMR CNRS 6074)\nContact : laetitia.chapel@agrocampus-ouest.fr\n\n\n\n\n\n√âtudiants participants :\n\nLes √©tudiants de la promotion 2025 du parcours Sciences des Donn√©es sont pr√©sent√©s dans la rubrique √Ä propos du site, avec leurs photos, noms et liens vers GitHub et LinkedIn. \n\n\nProgramme de la journ√©e\nLe programme complet, incluant les horaires de passages et les slides de chaque groupe, est consultable dans la rubrique Programme.\nTous les travaux des √©tudiants sont centralis√©s dans un d√©p√¥t GitHub. Chaque projet comprend le code, un document de synth√®se et les diapositives de pr√©sentation, permettant de consulter, reproduire et approfondir les travaux pr√©sent√©s durant cette √©dition de l‚Äô√©cole d‚Äôautomne.\n\n\nüìç Informations pratiques\nD√©tails d‚Äôacc√®s et plan disponibles dans la page Infos pratiques.\nLieu : Institut Agro Rennes, Amphi Matagrin\nHoraires : 9h00 ‚Äì 17h30."
  },
  {
    "objectID": "index.html#edition-2025-ecole-dautomne",
    "href": "index.html#edition-2025-ecole-dautomne",
    "title": "3·µâ √©dition de la conf√©rence de Machine Learning en Sciences du Vivant",
    "section": "Edition 2025 : Ecole d‚Äôautomne",
    "text": "Edition 2025 : Ecole d‚Äôautomne\nPour cette 3√®me √©dition, la conf√©rence prend le format d‚Äôune √©cole d‚Äôautomne.\nOrganis√©e par les √©tudiants de Master 2 et les √©l√®ves ing√©nieurs en sp√©cialisation Sciences des Donn√©es, cette √©cole d‚Äôautomne annuelle est un rendez-vous incontournable !\nElle s‚Äôinscrit dans le cadre de l‚Äô√©valuation des modules Machine Learning et Computer Science for Big Data Approaches, et permet aux √©tudiants de restituer leurs projets sous forme de mini‚Äëcours.\nC‚Äôest l‚Äôoccasion de mettre √† l‚Äôhonneur les travaux des √©tudiants et de d√©couvrir des approches modernes de Machine Learning et de Deep Learning appliqu√©es aux probl√©matiques des sciences du vivant.\n\nTh√®me 2025\nPour cette 3·µâ √©dition, nous explorons les apports du Machine Learning √† la compr√©hension, la mod√©lisation et la pr√©diction dans le champ du vivant : agriculture, environnement, √©cologie, sant√© ou biotechnologies.\nLes √©tudiants pr√©senteront des projets originaux illustrant la diversit√© des approches possibles et l‚Äôimpact concret de telles applications.\n\n\nProgramme de la journ√©e\nLe programme complet, incluant les horaires de passages et les slides de chaque groupe, est consultable dans la rubrique Programme.\nTous les travaux des √©tudiants sont centralis√©s dans un d√©p√¥t GitHub. Chaque projet comprend le code, un document de synth√®se et les diapositives de pr√©sentation, permettant de consulter, reproduire et approfondir les travaux pr√©sent√©s durant cette √©dition de l‚Äô√©cole d‚Äôautomne.\n\n\nüìç Informations pratiques\nD√©tails d‚Äôacc√®s et plan disponibles dans la page Infos pratiques.\nLieu : Salle 5√®me ann√©e du B√¢timent 24 de l‚ÄôInstitut Agro Rennes-Angers (Campus de Rennes)\nHoraires : Vendredi 21 novembre 2025 entre 13h30 et 16h30."
  },
  {
    "objectID": "projets/projets_val.html",
    "href": "projets/projets_val.html",
    "title": "Le Machine Learning dans l‚Äôagriculture",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/projets_val.html#lien-vers-le-code-github",
    "href": "projets/projets_val.html#lien-vers-le-code-github",
    "title": "Le Machine Learning dans l‚Äôagriculture",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/projets_val.html#lien-vers-les-slides",
    "href": "projets/projets_val.html#lien-vers-les-slides",
    "title": "Le Machine Learning dans l‚Äôagriculture",
    "section": "Lien vers les slides",
    "text": "Lien vers les slides\n\nSlides"
  },
  {
    "objectID": "projets/projets_val.html#description-projet",
    "href": "projets/projets_val.html#description-projet",
    "title": "Le Machine Learning dans l‚Äôagriculture",
    "section": "description Projet",
    "text": "description Projet\nNotre projet vise √† optimiser l‚Äôirrigation de plants de tomates gr√¢ce √† un agent d‚Äôapprentissage par renforcement apprenant √† d√©cider de la quantit√© d‚Äôeau √† fournir. Nous avons construit un environnement simulant l‚Äô√©volution de l‚Äôhumidit√© du sol et du rendement, puis entra√Æn√© un agent √† arroser les plants pour maximiser la production tout en r√©duisant la consommation d‚Äôeau. L‚Äôobjectif est de proposer une strat√©gie d‚Äôirrigation efficace et durable qui peut s‚Äôadapter √† diff√©rentes conditions observ√©es."
  },
  {
    "objectID": "projets/presentationFruits.html",
    "href": "projets/presentationFruits.html",
    "title": "Classification d‚Äôimages - une s√©paration entre R et Python",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/presentationFruits.html#lien-vers-le-code-github",
    "href": "projets/presentationFruits.html#lien-vers-le-code-github",
    "title": "Classification d‚Äôimages - une s√©paration entre R et Python",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/presentationFruits.html#lien-vers-les-slides",
    "href": "projets/presentationFruits.html#lien-vers-les-slides",
    "title": "Classification d‚Äôimages - une s√©paration entre R et Python",
    "section": "Lien vers les slides",
    "text": "Lien vers les slides\n\nSlides"
  },
  {
    "objectID": "projets/presentationFruits.html#description-projet",
    "href": "projets/presentationFruits.html#description-projet",
    "title": "Classification d‚Äôimages - une s√©paration entre R et Python",
    "section": "description Projet",
    "text": "description Projet\nLes disciplines de machine learning et de deep learning se pratiquent essentiellement avec le langage Python, langage de programmation le plus demand√© par les recruteurs de la Data Science en 2021 (KDnuggets). Or, le langage de programmation Open Source R est celui que nous utilisons le plus pour programmer.\nAinsi, nous nous sommes donn√© le challenge suivant : construire un mod√®le de deep learning en langage R.\nNous nous sommes demand√© : Comment un m√™me projet de deep learning peut-il √™tre abord√© diff√©remment selon l‚Äôoutil utilis√© ? Quels sont les avantages et limites de R Keras, Python Keras et PyTorch pour construire, entra√Æner et exploiter un mod√®le ?\nNotre exemple se place dans le cadre d‚Äôune classification de fruits pourris ou frais."
  },
  {
    "objectID": "projets/presentation_projet_CNN-BNN_insects.html",
    "href": "projets/presentation_projet_CNN-BNN_insects.html",
    "title": "Apport d‚Äôun mod√®le Bay√©sien lors de l‚Äôutilisation de r√©seaux de neuronnes pour la classification d‚Äôimages",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/presentation_projet_CNN-BNN_insects.html#lien-vers-le-code-github",
    "href": "projets/presentation_projet_CNN-BNN_insects.html#lien-vers-le-code-github",
    "title": "Apport d‚Äôun mod√®le Bay√©sien lors de l‚Äôutilisation de r√©seaux de neuronnes pour la classification d‚Äôimages",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/presentation_projet_CNN-BNN_insects.html#lien-vers-les-slides",
    "href": "projets/presentation_projet_CNN-BNN_insects.html#lien-vers-les-slides",
    "title": "Apport d‚Äôun mod√®le Bay√©sien lors de l‚Äôutilisation de r√©seaux de neuronnes pour la classification d‚Äôimages",
    "section": "Lien vers les slides",
    "text": "Lien vers les slides\n\nSlides"
  },
  {
    "objectID": "projets/presentation_projet_CNN-BNN_insects.html#description-projet",
    "href": "projets/presentation_projet_CNN-BNN_insects.html#description-projet",
    "title": "Apport d‚Äôun mod√®le Bay√©sien lors de l‚Äôutilisation de r√©seaux de neuronnes pour la classification d‚Äôimages",
    "section": "Description Projet",
    "text": "Description Projet\nNotre projet s‚Äôaxe sur des probl√©matiques d‚Äôapprentissage profond via r√©seaux de neuronnes, dans le cadre de la reconnaissance d‚Äôimages. Il a pour but l‚Äô√©valuation de l‚Äôapport d‚Äôun mod√®le comportant des composantes bay√©siennes par rapport √† un mod√®le purement fr√©quentiste. En comparant les 2 mod√®les du point de vue de l‚Äôacuracy en fonction des ressources demand√©es (nombre d‚Äôimage dans le jeu de donn√©e, temps d‚Äôentrainement du mod√®le, ressource GPU, ‚Ä¶) nous pourrons alors d√©terminer dans quels cas il est pr√©f√©rable d‚Äôutiliser l‚Äôune des 2 m√©thodes.\nEn terme de donn√©e, nous avons trouv√© un jeu de donn√©e ‚ÄúInsect_Detect_classification_v2‚Äù contenant des images d‚Äôinsectes de basse qualit√© (70x70) r√©partie dans 27 classes.\nPour effectuer cette d√©marche, le choix du mod√®le de CNN s‚Äôest d‚Äôabord impos√©. Notre r√©flexion s‚Äôest port√©e autour 3 mod√®les connus et √©prouv√©s : AlexNet, VGGNet et ResNet. Ces 3 mod√®les ont √©t√© entrain√© avec les data d‚Äô‚ÄúImageNet‚Äù, banque de plus de 14 millions d‚Äôimages annot√©es √† la main(1.2M √† l‚Äô√©poque). La simplicit√© de manipulation du mod√®le ainsi que sa clart√© ont √©t√© les principaux √©l√©ments de d√©cisions qui nous a pouss√© √† utiliser AlexNet, qui fait figure de basique du genre. En effet, ce mod√®le contient 8 couches : 5 convolutionnelles (+ 3 max-pooling) et 3 couches denses.\nNous poss√©dons des ressources limit√©es, et avons donc fait le choix de prendre le mod√®le AlexNet d√©j√† pr√©-entrain√© (en conservant les poids optenu √† la suite de son entrainement sur ImageNet) et de le fine-tuner pour nos donn√©es.\nAinsi, reste le probl√®me du mod√®le bay√©sien. Au cours de nos recherches, nous avons d√©couvert un package permettant de fine-tuner AlexNet en remplacant les couches convolutionnelles et lin√©aires classiques, qui renvoient des poids w fixe, en couches bay√©siennes qui renvoient une distriution de poids w.\nL‚Äôobjectif est alors de comprendre ce package, r√©ussir √† le faire fonctionner correctement pour notre probl√©matique, et l‚Äôoptimiser au possible avec nos ressources disponibles."
  },
  {
    "objectID": "projets/Projet_DeepMeow.html",
    "href": "projets/Projet_DeepMeow.html",
    "title": "Deep Meow",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/Projet_DeepMeow.html#lien-vers-le-code-github",
    "href": "projets/Projet_DeepMeow.html#lien-vers-le-code-github",
    "title": "Deep Meow",
    "section": "",
    "text": "Github"
  },
  {
    "objectID": "projets/Projet_DeepMeow.html#lien-vers-les-slides",
    "href": "projets/Projet_DeepMeow.html#lien-vers-les-slides",
    "title": "Deep Meow",
    "section": "Lien vers les slides",
    "text": "Lien vers les slides\n\nSlides"
  },
  {
    "objectID": "projets/Projet_DeepMeow.html#contexte",
    "href": "projets/Projet_DeepMeow.html#contexte",
    "title": "Deep Meow",
    "section": "Contexte :",
    "text": "Contexte :\nLa labellisation d‚Äôaudio est une m√©thode fortement utilis√©e en machine learning, en agronomie, en sociologie, musique, etc‚Ä¶ Nous proposons donc de pr√©senter une d√©marche de traitement de signaux audio et de labellisation de ces signaux."
  },
  {
    "objectID": "projets/Projet_DeepMeow.html#notre-jeu-de-donn√©es",
    "href": "projets/Projet_DeepMeow.html#notre-jeu-de-donn√©es",
    "title": "Deep Meow",
    "section": "Notre jeu de donn√©es :",
    "text": "Notre jeu de donn√©es :\nNotre exemple sera la labellisation d‚Äôaudio de chat. Le jeu de donn√©es comprend 440 audios au format .WAV. Chaque audio √©tant un miaulement de chat. Les chats sont enregistr√©s dans diff√©rentes situations (En train d‚Äô√™tre bross√©, attendant de la nourriture, isolement dans un espace inconnu). Le but ici est de d√©terminer le contexte d‚Äô√©mission d‚Äôun miaulement."
  },
  {
    "objectID": "projets/Projet_DeepMeow.html#points-abord√©s",
    "href": "projets/Projet_DeepMeow.html#points-abord√©s",
    "title": "Deep Meow",
    "section": "Points abord√©s :",
    "text": "Points abord√©s :\n\nPreprocessing sur des donn√©es audios\nExtraction de features via un r√©seau de neurones et des couches convolutionnelles"
  }
]